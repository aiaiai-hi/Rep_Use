# app.py
import json
import streamlit as st
import streamlit.components.v1 as components
import pandas as pd
import numpy as np
from io import BytesIO

# –ü–æ–∏—Å–∫ –ø–æ –ø–æ–ª—è–º
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# –õ–∏–Ω–µ–π–Ω–æ—Å—Ç—å
from pyvis.network import Network
import plotly.graph_objects as go

st.set_page_config(page_title="DWH ‚Üí –ü–æ–¥–±–æ—Ä –ø–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º, –ø—Ä–æ—Ç–æ—Ç–∏–ø –∏ –ª–∏–Ω–µ–π–Ω–æ—Å—Ç—å", layout="wide")

# ============================ –£–¢–ò–õ–ò–¢–´ =========================================
def df_to_excel_bytes(df: pd.DataFrame, sheet_name="data") -> bytes:
    output = BytesIO()
    with pd.ExcelWriter(output, engine="openpyxl") as writer:
        df.to_excel(writer, index=False, sheet_name=sheet_name)
    return output.getvalue()

def download_button_for_df(df, label, filename):
    st.download_button(label=label, data=df_to_excel_bytes(df),
                       file_name=filename, mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet")

# ---------------- TF-IDF –ø–æ–∏—Å–∫–æ–≤—ã–π –∏–Ω–¥–µ–∫—Å ----------------
def build_search_text(row, datasets):
    # –±–µ–∑–æ–ø–∞—Å–Ω–æ –Ω–∞ —Å–ª—É—á–∞–π –ø—Ä–æ–ø—É—Å–∫–æ–≤
    try:
        ds = datasets[datasets.dataset_id == row["dataset_id"]].iloc[0]
        ds_name = str(ds.get("name",""))
        ds_system = str(ds.get("system",""))
    except Exception:
        ds_name, ds_system = "", ""
    tags = row.get("tags", [])
    if isinstance(tags, str):
        tags = [t.strip() for t in tags.split(",")] if tags else []
    return " ".join(filter(None, [
        str(row.get("business_field_name","")),
        str(row.get("business_algorithm","")),
        str(row.get("column","")),
        " ".join(tags),
        f"{row.get('schema','')}.{row.get('table','')}",
        ds_name,
        ds_system
    ]))

def build_search_index(dataset_fields, datasets):
    df = dataset_fields.copy()
    if df.empty:
        # –ø—É—Å—Ç–æ–π –∏–Ω–¥–µ–∫—Å
        vec = TfidfVectorizer()
        tfidf = vec.fit_transform([""])
        return vec, tfidf
    df["search_text"] = df.apply(lambda r: build_search_text(r, datasets), axis=1)
    vectorizer = TfidfVectorizer(ngram_range=(1,2), analyzer="word", min_df=1)
    tfidf = vectorizer.fit_transform(df["search_text"].astype(str).values)
    return vectorizer, tfidf

def search_fields(query: str, dataset_fields, vectorizer, tfidf, top_k: int = 30):
    if not query.strip() or len(dataset_fields)==0:
        return []
    q_vec = vectorizer.transform([query])
    sim = cosine_similarity(q_vec, tfidf).ravel()
    idx = np.argsort(sim)[::-1][:top_k]
    return [(dataset_fields.iloc[i]["ref"], float(sim[i])) for i in idx if sim[i] > 0]

# ---------------- –õ–∏–Ω–µ–π–Ω–æ—Å—Ç—å ----------------
def build_lineage_edges(dataset_fields: pd.DataFrame, report_fields: pd.DataFrame):
    """
    dataset(schema.table) -> field(schema.table.column) -> report(report:<id>)
    """
    df = dataset_fields.copy()
    df["dataset"] = df["schema"].astype(str) + "." + df["table"].astype(str)
    edges = []
    # dataset -> field
    for _, r in df.iterrows():
        ds = r["dataset"]
        field_ref = f"{r['schema']}.{r['table']}.{r['column']}"
        edges.append((ds, field_ref, "dataset‚Üífield"))
    # field -> report
    for _, r in report_fields.iterrows():
        field_ref = r["source_ref"]
        rep = f"report:{r['report_id']}"
        edges.append((field_ref, rep, "field‚Üíreport"))
    return edges

def pyvis_graph(edges, reports: pd.DataFrame, datasets: pd.DataFrame, height="650px"):
    g = Network(height=height, width="100%", bgcolor="#FFFFFF", font_color="#111111", notebook=False, directed=True)
    try:
        g.barnes_hut(gravity=-20000, central_gravity=0.1, spring_length=150, spring_strength=0.01)
    except Exception:
        pass

    ds_set = set(datasets["name"].astype(str).tolist())
    report_meta = {f"report:{r.report_id}": r.name for _, r in reports.iterrows()}

    def node_style(n):
        if n in ds_set:             # dataset
            return dict(color="#1f77b4", shape="box")
        if isinstance(n, str) and n.startswith("report:"): # report
            return dict(color="#2ca02c", shape="box")
        if isinstance(n, str) and n.count(".") == 2:       # field
            return dict(color="#ff7f0e", shape="ellipse")
        return dict(color="#7f7f7f", shape="dot")

    nodes = set()
    for s, t, lbl in edges:
        for n in (s, t):
            if n not in nodes:
                style = node_style(n)
                title = n
                if n in report_meta:
                    title = f"{n} | {report_meta[n]}"
                label = n.split(".")[-1] if isinstance(n, str) and n.count(".")>=1 else (report_meta.get(n, str(n)))
                g.add_node(n, label=label, title=title, **style)
                nodes.add(n)
        g.add_edge(s, t, title=lbl, arrows="to")

    options = {
        "nodes": {"borderWidth": 1, "size": 18},
        "edges": {"color": {"color": "#B3B3B3"}, "smooth": {"type": "dynamic"}},
        "physics": {"stabilization": True}
    }
    try:
        g.set_options(json.dumps(options, ensure_ascii=False))
    except Exception:
        pass
    return g

def sankey_figure(edges):
    nodes = sorted(set([s for s,_,_ in edges] + [t for _,t,_ in edges]))
    idx = {n:i for i,n in enumerate(nodes)}
    source = [idx[s] for s,_,_ in edges]
    target = [idx[t] for _,t,_ in edges]
    value  = [1 for _ in edges]
    fig = go.Figure(data=[go.Sankey(
        node=dict(pad=15, thickness=20, line=dict(width=0.5), label=[str(n) for n in nodes]),
        link=dict(source=source, target=target, value=value)
    )])
    fig.update_layout(height=650, margin=dict(l=10,r=10,t=10,b=10))
    return fig

def filter_edges_by_report(edges, report_id: int | None):
    if report_id is None:
        return edges
    rnode = f"report:{report_id}"
    keep = set([rnode])
    changed = True
    while changed:
        changed = False
        for s,t,_ in edges:
            if t in keep and s not in keep:
                keep.add(s); changed = True
    return [(s,t,l) for s,t,l in edges if s in keep and t in keep]

def filter_edges_by_report_name(edges, reports_df: pd.DataFrame, report_name: str | None):
    """–§–∏–ª—å—Ç—Ä—É–µ—Ç —Ä—ë–±—Ä–∞ –¥–ª—è –≤—ã–±—Ä–∞–Ω–Ω–æ–≥–æ –æ—Ç—á—ë—Ç–∞ –ø–æ –∏–º–µ–Ω–∏; None ‚Üí –≤—Å–µ —Ä—ë–±—Ä–∞."""
    if not report_name or report_name.strip() in {"‚Äî –í—Å–µ –æ—Ç—á—ë—Ç—ã ‚Äî"}:
        return edges
    row = reports_df[reports_df["name"] == report_name]
    if row.empty:
        return edges
    report_id = int(row.iloc[0]["report_id"])
    return filter_edges_by_report(edges, report_id)

# ====================== –¢–ï–°–¢–û–í–´–ï –î–ê–ù–ù–´–ï (–º–æ–∂–Ω–æ –∑–∞–º–µ–Ω–∏—Ç—å) ======================
def load_default_frames():
    datasets = pd.DataFrame([
        {"dataset_id":10,"name":"dm.sales_facts","layer":"vitrine","owner":"DWH","sla_minutes":120,"pii_flags":"","quality_score":0.93,"granularity":"txn_day_sku","system":"DWH / Sales Mart"},
        {"dataset_id":11,"name":"dm.customer_facts","layer":"vitrine","owner":"DWH","sla_minutes":1440,"pii_flags":"PII","quality_score":0.88,"granularity":"customer_month","system":"DWH / CRM Mart"},
        {"dataset_id":12,"name":"raw.erp_costs","layer":"raw","owner":"DataOps","sla_minutes":60,"pii_flags":"","quality_score":0.76,"granularity":"sku_day","system":"ERP"},
        {"dataset_id":13,"name":"dm.finance_facts","layer":"vitrine","owner":"DWH","sla_minutes":1440,"pii_flags":"","quality_score":0.86,"granularity":"dept_month","system":"DWH / Finance Mart"},
        {"dataset_id":14,"name":"raw.crm_events","layer":"raw","owner":"MarTech","sla_minutes":30,"pii_flags":"PII","quality_score":0.71,"granularity":"event","system":"CRM"},
        {"dataset_id":15,"name":"dm.geo_dim","layer":"vitrine","owner":"DWH","sla_minutes":1440,"pii_flags":"","quality_score":0.95,"granularity":"region","system":"DWH / Master Data"},
        {"dataset_id":16,"name":"dm.sales_dim","layer":"vitrine","owner":"DWH","sla_minutes":1440,"pii_flags":"","quality_score":0.92,"granularity":"channel","system":"DWH / Master Data"},
        {"dataset_id":17,"name":"dm.customer_dim","layer":"vitrine","owner":"DWH","sla_minutes":1440,"pii_flags":"PII","quality_score":0.90,"granularity":"customer","system":"DWH / Master Data"},
    ])
    reports = pd.DataFrame([
        {"report_id":1,"name":"–ü—Ä–æ–¥–∞–∂–∏ –ø–æ —Ä–µ–≥–∏–æ–Ω–∞–º","owner":"BI Team","frequency":"–ï–∂–µ–¥–Ω–µ–≤–Ω–æ","business_domain":"Sales","is_automated":True,"automation_score":92,"description":"–í–æ—Ä–æ–Ω–∫–∞ –ø—Ä–æ–¥–∞–∂ –∏ –≤—ã—Ä—É—á–∫–∞ –ø–æ —Ä–µ–≥–∏–æ–Ω–∞–º –∏ –∫–∞–Ω–∞–ª–∞–º."},
        {"report_id":2,"name":"–ú–∞—Ä–∂–∏–Ω–∞–ª—å–Ω–æ—Å—Ç—å SKU (—Ä—É—á–Ω–æ–π)","owner":"Finance","frequency":"–ï–∂–µ–Ω–µ–¥–µ–ª—å–Ω–æ","business_domain":"Finance","is_automated":False,"automation_score":58,"description":"–†—É—á–Ω–æ–π excel –ø–æ –º–∞—Ä–∂–µ –∏ —Å–µ–±–µ—Å—Ç–æ–∏–º–æ—Å—Ç–∏ –Ω–∞ —É—Ä–æ–≤–Ω–µ SKU."},
        {"report_id":3,"name":"Churn –¥–∞—à–±–æ—Ä–¥","owner":"CRM","frequency":"–ï–∂–µ–º–µ—Å—è—á–Ω–æ","business_domain":"CRM","is_automated":True,"automation_score":81,"description":"–û—Ç—Ç–æ–∫ –∫–ª–∏–µ–Ω—Ç–æ–≤, —Ä–µ—Ç–µ–Ω—à–Ω –∏ —Å–µ–≥–º–µ–Ω—Ç—ã."},
        {"report_id":4,"name":"–ü–ª–∞–Ω/–§–∞–∫—Ç –î–æ—Ö–æ–¥–æ–≤","owner":"FP&A","frequency":"–ï–∂–µ–º–µ—Å—è—á–Ω–æ","business_domain":"Finance","is_automated":True,"automation_score":76,"description":"–°–≤–æ–¥ –¥–æ—Ö–æ–¥–æ–≤ –ø—Ä–æ—Ç–∏–≤ –±—é–¥–∂–µ—Ç–∞ –ø–æ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è–º."},
    ])
    report_fields = pd.DataFrame([
        {"report_id":1,"business_field_name":"–í—ã—Ä—É—á–∫–∞","business_algorithm":"SUM(price*qty) –ø–æ –¥–Ω—é/sku","source_ref":"dm.sales_facts.revenue","is_from_vitrine":True},
        {"report_id":1,"business_field_name":"–ö–∞–Ω–∞–ª –ø—Ä–æ–¥–∞–∂","business_algorithm":"–°–ø—Ä–∞–≤–æ—á–Ω–∏–∫ –∫–∞–Ω–∞–ª–æ–≤","source_ref":"dm.sales_dim.channel","is_from_vitrine":True},
        {"report_id":1,"business_field_name":"–†–µ–≥–∏–æ–Ω","business_algorithm":"–°–ø—Ä–∞–≤–æ—á–Ω–∏–∫ –≥–µ–æ–≥—Ä–∞—Ñ–∏–π","source_ref":"dm.geo_dim.region_name","is_from_vitrine":True},
        {"report_id":2,"business_field_name":"SKU","business_algorithm":"–ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä —Ç–æ–≤–∞—Ä–∞ (ERP)","source_ref":"raw.erp_costs.sku","is_from_vitrine":False},
        {"report_id":2,"business_field_name":"–°–µ–±–µ—Å—Ç–æ–∏–º–æ—Å—Ç—å","business_algorithm":"–°—É–º–º–∞ –ø—Ä—è–º—ã—Ö –∑–∞—Ç—Ä–∞—Ç (ERP)","source_ref":"raw.erp_costs.cogs","is_from_vitrine":False},
        {"report_id":2,"business_field_name":"–¶–µ–Ω–∞ –ø—Ä–æ–¥–∞–∂–∏","business_algorithm":"–¶–µ–Ω–∞ –∏–∑ —Ñ–∞–∫—Ç–æ–≤ –ø—Ä–æ–¥–∞–∂","source_ref":"dm.sales_facts.price","is_from_vitrine":True},
        {"report_id":3,"business_field_name":"–ö–ª–∏–µ–Ω—Ç","business_algorithm":"–ö–ª—é—á –∫–ª–∏–µ–Ω—Ç–∞ (Master)","source_ref":"dm.customer_dim.customer_id","is_from_vitrine":True},
        {"report_id":3,"business_field_name":"–°—Ç–∞—Ç—É—Å –æ—Ç—Ç–æ–∫–∞","business_algorithm":"–§–ª–∞–≥ churn (–ª–æ–≥–∏–∫–∞ CRM)","source_ref":"dm.customer_facts.churn_flag","is_from_vitrine":True},
        {"report_id":3,"business_field_name":"–î–∞—Ç–∞ –ø–æ—Å–ª–µ–¥–Ω–µ–π –ø–æ–∫—É–ø–∫–∏","business_algorithm":"Max(order_dt) –ø–æ –∫–ª–∏–µ–Ω—Ç—É","source_ref":"dm.customer_facts.last_purchase_dt","is_from_vitrine":True},
        {"report_id":4,"business_field_name":"–î–æ—Ö–æ–¥ —Ñ–∞–∫—Ç","business_algorithm":"–§–∞–∫—Ç –¥–æ—Ö–æ–¥–æ–≤ –∑–∞ –ø–µ—Ä–∏–æ–¥","source_ref":"dm.finance_facts.revenue_actual","is_from_vitrine":True},
        {"report_id":4,"business_field_name":"–î–æ—Ö–æ–¥ –ø–ª–∞–Ω","business_algorithm":"–ë—é–¥–∂–µ—Ç–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –¥–æ—Ö–æ–¥–æ–≤","source_ref":"dm.finance_facts.revenue_budget","is_from_vitrine":True},
    ])
    dataset_fields = pd.DataFrame([
        {"dataset_id":10,"schema":"dm","table":"sales_facts","column":"revenue","dtype":"decimal","completeness":0.99,"uniqueness":0.95,"tags":["–≤—ã—Ä—É—á–∫–∞","–¥–æ—Ö–æ–¥","–æ–±–æ—Ä–æ—Ç","revenue","sales"],"business_field_name":"–í—ã—Ä—É—á–∫–∞","business_algorithm":"SUM(price*qty)"},
        {"dataset_id":10,"schema":"dm","table":"sales_facts","column":"price","dtype":"decimal","completeness":0.98,"uniqueness":0.92,"tags":["—Ü–µ–Ω–∞","price","—Å—Ç–æ–∏–º–æ—Å—Ç—å –ø—Ä–æ–¥–∞–∂–∏"],"business_field_name":"–¶–µ–Ω–∞ –ø—Ä–æ–¥–∞–∂–∏","business_algorithm":"price –ø–æ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏"},
        {"dataset_id":10,"schema":"dm","table":"sales_facts","column":"sku_id","dtype":"string","completeness":0.97,"uniqueness":0.80,"tags":["sku","—Ç–æ–≤–∞—Ä","–∞—Ä—Ç–∏–∫—É–ª"],"business_field_name":"SKU","business_algorithm":"–ö–ª—é—á SKU"},
        {"dataset_id":10,"schema":"dm","table":"sales_facts","column":"channel","dtype":"string","completeness":0.98,"uniqueness":0.70,"tags":["–∫–∞–Ω–∞–ª","–æ–Ω–ª–∞–π–Ω","–æ—Ñ—Ñ–ª–∞–π–Ω","—Ä–æ–∑–Ω–∏—Ü–∞","ecom"],"business_field_name":"–ö–∞–Ω–∞–ª –ø—Ä–æ–¥–∞–∂","business_algorithm":"–°–ø—Ä–∞–≤–æ—á–Ω–∏–∫ –∫–∞–Ω–∞–ª–æ–≤"},
        {"dataset_id":10,"schema":"dm","table":"sales_facts","column":"region_id","dtype":"int","completeness":0.98,"uniqueness":0.60,"tags":["—Ä–µ–≥–∏–æ–Ω","–≥–µ–æ","–æ–±–ª–∞—Å—Ç—å"],"business_field_name":"–†–µ–≥–∏–æ–Ω ID","business_algorithm":"–°—Å—ã–ª–∫–∞ –Ω–∞ geo_dim"},
        {"dataset_id":11,"schema":"dm","table":"customer_facts","column":"churn_flag","dtype":"bool","completeness":0.97,"uniqueness":1.00,"tags":["–æ—Ç—Ç–æ–∫","churn","—É—à—ë–ª","—É–¥–µ—Ä–∂–∞–Ω–∏–µ"],"business_field_name":"–°—Ç–∞—Ç—É—Å –æ—Ç—Ç–æ–∫–∞","business_algorithm":"ML/–ø—Ä–∞–≤–∏–ª–æ churn"},
        {"dataset_id":11,"schema":"dm","table":"customer_facts","column":"last_purchase_dt","dtype":"date","completeness":0.96,"uniqueness":0.90,"tags":["–ø–æ—Å–ª–µ–¥–Ω—è—è –ø–æ–∫—É–ø–∫–∞","recency","lrp"],"business_field_name":"–î–∞—Ç–∞ –ø–æ—Å–ª–µ–¥–Ω–µ–π –ø–æ–∫—É–ø–∫–∏","business_algorithm":"Max(order_dt)"},
        {"dataset_id":13,"schema":"dm","table":"finance_facts","column":"revenue_actual","dtype":"decimal","completeness":0.98,"uniqueness":0.95,"tags":["–¥–æ—Ö–æ–¥ —Ñ–∞–∫—Ç","—Ñ–∞–∫—Ç","actual","–≤—ã—Ä—É—á–∫–∞"],"business_field_name":"–î–æ—Ö–æ–¥ —Ñ–∞–∫—Ç","business_algorithm":"–§–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –¥–æ—Ö–æ–¥—ã"},
        {"dataset_id":13,"schema":"dm","table":"finance_facts","column":"revenue_budget","dtype":"decimal","completeness":0.98,"uniqueness":0.95,"tags":["–ø–ª–∞–Ω –¥–æ—Ö–æ–¥","–±—é–¥–∂–µ—Ç","budget"],"business_field_name":"–î–æ—Ö–æ–¥ –ø–ª–∞–Ω","business_algorithm":"–ë—é–¥–∂–µ—Ç –¥–æ—Ö–æ–¥–æ–≤"},
        {"dataset_id":15,"schema":"dm","table":"geo_dim","column":"region_name","dtype":"string","completeness":0.99,"uniqueness":0.95,"tags":["—Ä–µ–≥–∏–æ–Ω","–≥–µ–æ–≥—Ä–∞—Ñ–∏—è","—Ä–µ–≥–∏–æ–Ω –Ω–∞–∑–≤–∞–Ω–∏–µ"],"business_field_name":"–†–µ–≥–∏–æ–Ω","business_algorithm":"–°–ø—Ä–∞–≤–æ—á–Ω–∏–∫ –≥–µ–æ–≥—Ä–∞—Ñ–∏–π"},
        {"dataset_id":16,"schema":"dm","table":"sales_dim","column":"channel","dtype":"string","completeness":0.99,"uniqueness":0.95,"tags":["–∫–∞–Ω–∞–ª","–∫–∞–Ω–∞–ª –ø—Ä–æ–¥–∞–∂","—Ä–æ–∑–Ω–∏—Ü–∞","marketplace"],"business_field_name":"–ö–∞–Ω–∞–ª –ø—Ä–æ–¥–∞–∂","business_algorithm":"–°–ø—Ä–∞–≤–æ—á–Ω–∏–∫ –∫–∞–Ω–∞–ª–æ–≤"},
        {"dataset_id":17,"schema":"dm","table":"customer_dim","column":"customer_id","dtype":"string","completeness":0.99,"uniqueness":1.00,"tags":["–∫–ª–∏–µ–Ω—Ç","customer","–∏–¥ –∫–ª–∏–µ–Ω—Ç–∞"],"business_field_name":"–ö–ª–∏–µ–Ω—Ç","business_algorithm":"Master ID"},
        {"dataset_id":12,"schema":"raw","table":"erp_costs","column":"cogs","dtype":"decimal","completeness":0.92,"uniqueness":0.90,"tags":["—Å–µ–±–µ—Å—Ç–æ–∏–º–æ—Å—Ç—å","cogs","–∑–∞—Ç—Ä–∞—Ç—ã"],"business_field_name":"–°–µ–±–µ—Å—Ç–æ–∏–º–æ—Å—Ç—å","business_algorithm":"ERP COGS"},
        {"dataset_id":12,"schema":"raw","table":"erp_costs","column":"sku","dtype":"string","completeness":0.94,"uniqueness":0.80,"tags":["sku","—Ç–æ–≤–∞—Ä","–∞—Ä—Ç–∏–∫—É–ª"],"business_field_name":"SKU","business_algorithm":"ERP SKU"},
        {"dataset_id":14,"schema":"raw","table":"crm_events","column":"event_type","dtype":"string","completeness":0.91,"uniqueness":0.65,"tags":["—Å–æ–±—ã—Ç–∏–µ","email","push","–∫–∞–º–ø–∞–Ω–∏—è"],"business_field_name":"–¢–∏–ø —Å–æ–±—ã—Ç–∏—è","business_algorithm":"CRM event type"},
    ])
    return datasets, reports, report_fields, dataset_fields

# ============================ SESSION STATE ===================================
if "datasets" not in st.session_state:
    st.session_state.datasets, st.session_state.reports, st.session_state.report_fields, st.session_state.dataset_fields = load_default_frames()
if "selected_refs" not in st.session_state:
    st.session_state.selected_refs = []
if "is_admin" not in st.session_state:
    st.session_state.is_admin = False

datasets = st.session_state.datasets
reports = st.session_state.reports
report_fields = st.session_state.report_fields
dataset_fields = st.session_state.dataset_fields

# –°–ª—É–∂–µ–±–Ω—ã–µ –ø–æ–ª—è
if {"schema","table","column"}.issubset(dataset_fields.columns):
    dataset_fields["ref"] = dataset_fields["schema"].astype(str) + "." + dataset_fields["table"].astype(str) + "." + dataset_fields["column"].astype(str)
else:
    dataset_fields["ref"] = ""  # –Ω–∞ —Å–ª—É—á–∞–π –Ω–µ–ø–æ–ª–Ω—ã—Ö –∑–∞–≥—Ä—É–∑–æ–∫
ref_to_dataset = {r["ref"]: r["dataset_id"] for _, r in dataset_fields.iterrows() if "dataset_id" in r and pd.notna(r["ref"])}

# –°—Ç—Ä–æ–∏–º/–ø–µ—Ä–µ—Å—Ç—Ä–∞–∏–≤–∞–µ–º –ø–æ–∏—Å–∫–æ–≤—ã–π –∏–Ω–¥–µ–∫—Å (–ø—Ä–∏ –ø–µ—Ä–≤–æ–º –∑–∞–ø—É—Å–∫–µ)
if "vectorizer" not in st.session_state or "tfidf" not in st.session_state:
    st.session_state.vectorizer, st.session_state.tfidf = build_search_index(dataset_fields, datasets)

vectorizer = st.session_state.vectorizer
tfidf = st.session_state.tfidf

# ================================== –û–î–ù–ê –°–¢–†–ê–ù–ò–¶–ê ===================================
st.title("–ö–∞—Ç–∞–ª–æ–≥ –¥–∞–Ω–Ω—ã—Ö –∏ –æ—Ç—á—ë—Ç–æ–≤ ‚Üí –ü–æ–¥–±–æ—Ä –ø–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º, –ø—Ä–æ—Ç–æ—Ç–∏–ø –∏ –ª–∏–Ω–µ–π–Ω–æ—Å—Ç—å")
st.caption("–ü–æ–¥–±–µ—Ä–∏—Ç–µ –ø–æ–ª—è –∏ —Å–æ–±–µ—Ä–∏—Ç–µ –ø—Ä–æ—Ç–æ—Ç–∏–ø –æ—Ç—á—ë—Ç–∞, –Ω–µ –∑–Ω–∞—è –∑–∞—Ä–∞–Ω–µ–µ —Å—Ö–µ–º—ã –∏ —Ç–∞–±–ª–∏—Ü—ã.")

# –¢–∞–±—ã –æ–¥–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
tab1, tab2, tab3, tab4, tab5, tab6 = st.tabs([
    "üîé –ü–æ–¥–±–æ—Ä –ø–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º",
    "üß± –í–∏—Ç—Ä–∏–Ω—ã",
    "üìã –†–µ–µ—Å—Ç—Ä –æ—Ç—á—ë—Ç–æ–≤",
    "üóÇÔ∏è –ê—Ç—Ä–∏–±—É—Ç—ã –æ—Ç—á—ë—Ç–∞",
    "üß≠ –õ–∏–Ω–µ–π–Ω–æ—Å—Ç—å",
    "üì• –ò–º–ø–æ—Ä—Ç/–≠–∫—Å–ø–æ—Ä—Ç"
])

# ------------------- TAB 1: –ü–æ–¥–±–æ—Ä –ø–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º -------------------------
with tab1:
    st.subheader("–û–ø–∏—à–∏—Ç–µ —á—Ç–æ –≤—ã —Ö–æ—Ç–∏—Ç–µ")
    st.caption("–í–≤–µ–¥–∏—Ç–µ –Ω–∞–∑–≤–∞–Ω–∏—è –ø–æ–∫–∞–∑–∞—Ç–µ–ª–µ–π, –∏–∑–º–µ—Ä–µ–Ω–∏–π, —Å–∏—Å—Ç–µ–º –∏–ª–∏ —Ç–∞–±–ª–∏—Ü (–Ω–∞–ø—Ä–∏–º–µ—Ä: ¬´–≤—ã—Ä—É—á–∫–∞¬ª, ¬´–∫–∞–Ω–∞–ª –ø—Ä–æ–¥–∞–∂¬ª, ¬´ERP —Å–µ–±–µ—Å—Ç–æ–∏–º–æ—Å—Ç—å¬ª).")

    qcol1, qcol2 = st.columns([3,1])
    with qcol1:
        q = st.text_input("–ü–æ–∏—Å–∫ –ø–æ –ø–æ–ª—è–º (—Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π TF-IDF)", placeholder="–ù–∞–ø—Ä–∏–º–µ—Ä: –≤—ã—Ä—É—á–∫–∞, –∫–∞–Ω–∞–ª –ø—Ä–æ–¥–∞–∂, —Ä–µ–≥–∏–æ–Ω, —Å–µ–±–µ—Å—Ç–æ–∏–º–æ—Å—Ç—å ...")
    with qcol2:
        if st.button("üîÑ –ü–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞—Ç—å —Å–µ–π—á–∞—Å"):
            st.session_state.vectorizer, st.session_state.tfidf = build_search_index(dataset_fields, datasets)
            vectorizer = st.session_state.vectorizer
            tfidf = st.session_state.tfidf
            st.success("–ò–Ω–¥–µ–∫—Å –ø–æ–∏—Å–∫–∞ –ø–µ—Ä–µ—Å—Ç—Ä–æ–µ–Ω.")

    cols = st.columns([3,1])
    with cols[0]:
        if q:
            results = search_fields(q, dataset_fields, vectorizer, tfidf, top_k=50)
            if results:
                st.write("–ù–∞–π–¥–µ–Ω–Ω—ã–µ –ø–æ–ª—è:")
                for ref, sc in results:
                    if ref not in ref_to_dataset:
                        continue
                    ds_id = ref_to_dataset[ref]
                    ds = datasets[datasets.dataset_id==ds_id].iloc[0] if not datasets[datasets.dataset_id==ds_id].empty else {}
                    row = dataset_fields.set_index("ref").loc[ref]
                    add_key = f"add_{ref}"
                    with st.container():
                        c1, c2, c3, c4, c5 = st.columns([3,2,1,1,1])
                        c1.markdown(f"**{row.get('business_field_name','')}**  \n`{ref}`")
                        c2.markdown(f"–°–∏—Å—Ç–µ–º–∞: **{ds.get('system','')}**  \n–ù–∞–±–æ—Ä: `{ds.get('name','')}`")
                        c3.markdown(f"–°–ª–æ–π: `{ds.get('layer','')}`")
                        c4.markdown(f"DQ: **{row.get('completeness',0):.2f}**")
                        c5.markdown(f"score: {sc:.2f}")
                        if st.button("–î–æ–±–∞–≤–∏—Ç—å –≤ –ø—Ä–æ—Ç–æ—Ç–∏–ø", key=add_key):
                            if ref not in st.session_state.selected_refs:
                                st.session_state.selected_refs.append(ref)
            else:
                st.info("–ù–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –¥—Ä—É–≥–∏–µ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏ –∏–ª–∏ –ø–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∏—Ä—É–π—Ç–µ.")
        else:
            st.caption("–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ—è–≤—è—Ç—Å—è –ø–æ—Å–ª–µ –≤–≤–æ–¥–∞ –ø–æ–∏—Å–∫–æ–≤–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞.")

    with cols[1]:
        if st.button("–û—á–∏—Å—Ç–∏—Ç—å –≤—ã–±—Ä–∞–Ω–Ω—ã–µ"):
            st.session_state.selected_refs = []

    st.markdown("---")
    st.markdown("### –ü—Ä–æ—Ç–æ—Ç–∏–ø –æ—Ç—á—ë—Ç–∞")
    if not st.session_state.selected_refs:
        st.info("–î–æ–±–∞–≤–ª—è–π—Ç–µ –ø–æ–ª—è –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞ ‚Äî –æ–Ω–∏ –ø–æ—è–≤—è—Ç—Å—è –≤ —Ç–∞–±–ª–∏—Ü–µ –Ω–∏–∂–µ.")
    else:
        # –¢–∞–±–ª–∏—Ü–∞ –ø—Ä–æ—Ç–æ—Ç–∏–ø–∞ —Å —Ç—Ä–µ–±—É–µ–º—ã–º–∏ –∫–æ–ª–æ–Ω–∫–∞–º–∏
        rows = []
        for i, ref in enumerate(st.session_state.selected_refs, start=1):
            if ref not in ref_to_dataset or ref not in dataset_fields.set_index("ref").index:
                continue
            ds_id = ref_to_dataset[ref]
            ds_row = datasets[datasets.dataset_id==ds_id]
            ds = ds_row.iloc[0] if not ds_row.empty else {}
            rf = report_fields[report_fields["source_ref"]==ref]
            used_ids = rf["report_id"].tolist()
            used_names = reports[reports["report_id"].isin(used_ids)]["name"].tolist()
            row = dataset_fields.set_index("ref").loc[ref]
            rows.append({
                "‚Ññ": i,
                "–ë–∏–∑–Ω–µ—Å-–ø–æ–ª–µ": row.get("business_field_name",""),
                "–ë–∏–∑–Ω–µ—Å-–∞–ª–≥–æ—Ä–∏—Ç–º": row.get("business_algorithm",""),
                "–ò—Å—Ç–æ—á–Ω–∏–∫ (schema.table.column)": ref,
                "–°–≤—è–∑—å —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–æ–π —Å–∏—Å—Ç–µ–º–æ–π": ds.get("system",""),
                "–í –∫–∞–∫—É—é —Ç–∞–±–ª–∏—Ü—É –≤—Ö–æ–¥–∏—Ç": f"{row.get('schema','')}.{row.get('table','')}",
                "–í –∫–∞–∫–∏—Ö –æ—Ç—á—ë—Ç–∞—Ö –µ—Å—Ç—å (ID)": ", ".join(map(str, used_ids)) if used_ids else "‚Äî",
                "–ù–∞–∑–≤–∞–Ω–∏—è –æ—Ç—á—ë—Ç–æ–≤": ", ".join(used_names) if used_names else "‚Äî",
            })
        df_proto = pd.DataFrame(rows)
        st.dataframe(df_proto, use_container_width=True, height=360)
        download_button_for_df(df_proto, "‚¨áÔ∏è –°–∫–∞—á–∞—Ç—å –ø—Ä–æ—Ç–æ—Ç–∏–ø (Excel)", "prototype.xlsx")

# ------------------- TAB 2: –í–∏—Ç—Ä–∏–Ω—ã --------------------------------------
with tab2:
    c1, c2 = st.columns(2)
    with c1:
        st.write("**–í–∏—Ç—Ä–∏–Ω–∞ (dm.*)**")
        st.dataframe(datasets.query("layer=='vitrine'")[["name","system","owner","sla_minutes","pii_flags","quality_score","granularity"]]
                     .rename(columns={"name":"–ù–∞–±–æ—Ä","system":"–°–∏—Å—Ç–µ–º–∞","owner":"–í–ª–∞–¥–µ–ª–µ—Ü","sla_minutes":"SLA (–º–∏–Ω)","pii_flags":"PII","quality_score":"–ö–∞—á–µ—Å—Ç–≤–æ","granularity":"–ì—Ä–∞–Ω—É–ª—è—Ä–Ω–æ—Å—Ç—å"}),
                     use_container_width=True, height=260)
    with c2:
        st.write("**–ò—Å—Ç–æ—á–Ω–∏–∫–∏ (RAW/Source)**")
        st.dataframe(datasets.query("layer!='vitrine'")[["name","system","owner","sla_minutes","pii_flags","quality_score","granularity"]]
                     .rename(columns={"name":"–ù–∞–±–æ—Ä","system":"–°–∏—Å—Ç–µ–º–∞","owner":"–í–ª–∞–¥–µ–ª–µ—Ü","sla_minutes":"SLA (–º–∏–Ω)","pii_flags":"PII","quality_score":"–ö–∞—á–µ—Å—Ç–≤–æ","granularity":"–ì—Ä–∞–Ω—É–ª—è—Ä–Ω–æ—Å—Ç—å"}),
                     use_container_width=True, height=260)

    st.markdown("### –ü–æ–ª—è")
    df_fields = dataset_fields.copy()
    if not df_fields.empty:
        df_fields.insert(0, "‚Ññ", range(1, len(df_fields)+1))
        df_fields = df_fields[["‚Ññ","business_field_name","business_algorithm","schema","table","column","dtype","completeness","uniqueness","tags"]]
        df_fields = df_fields.rename(columns={
            "business_field_name":"–ë–∏–∑–Ω–µ—Å-–ø–æ–ª–µ",
            "business_algorithm":"–ë–∏–∑–Ω–µ—Å-–∞–ª–≥–æ—Ä–∏—Ç–º",
            "schema":"–°—Ö–µ–º–∞","table":"–¢–∞–±–ª–∏—Ü–∞","column":"–ü–æ–ª–µ","dtype":"–¢–∏–ø",
            "completeness":"–ü–æ–ª–Ω–æ—Ç–∞","uniqueness":"–£–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å","tags":"–¢–µ–≥–∏"
        })
    st.dataframe(df_fields, use_container_width=True, height=360)

# ------------------- TAB 3: –†–µ–µ—Å—Ç—Ä –æ—Ç—á—ë—Ç–æ–≤ -----------------------------
with tab3:
    st.write("### –†–µ–µ—Å—Ç—Ä –æ—Ç—á—ë—Ç–æ–≤")
    query = st.text_input("–§–∏–ª—å—Ç—Ä –ø–æ –Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏—é/–≤–ª–∞–¥–µ–ª—å—Ü—É/–¥–æ–º–µ–Ω—É", "")
    show = reports.copy()
    if query:
        ql = query.lower()
        for col in ["name","owner","business_domain"]:
            if col not in show.columns:
                show[col] = ""
        mask = (
            show["name"].astype(str).str.lower().str.contains(ql) |
            show["owner"].astype(str).str.lower().str.contains(ql) |
            show["business_domain"].astype(str).str.lower().str.contains(ql)
        )
        show = show[mask]
    grid = show[["name","owner","business_domain","frequency","is_automated","automation_score","description"]].rename(
        columns={"name":"–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ","owner":"–í–ª–∞–¥–µ–ª–µ—Ü","business_domain":"–î–æ–º–µ–Ω","frequency":"–ß–∞—Å—Ç–æ—Ç–∞","is_automated":"–ê–≤—Ç–æ?","automation_score":"–°–∫–æ—Ä","description":"–û–ø–∏—Å–∞–Ω–∏–µ"})
    st.dataframe(grid, use_container_width=True, height=300)

# ------------------- TAB 4: –ê—Ç—Ä–∏–±—É—Ç—ã –æ—Ç—á—ë—Ç–∞ ----------------------------
with tab4:
    selected = st.selectbox("–í—ã–±–µ—Ä–∏—Ç–µ –æ—Ç—á—ë—Ç", options=reports["name"].astype(str).tolist())
    rep = reports[reports["name"]==selected].iloc[0]
    rid = rep["report_id"]
    st.write(f"**–í–ª–∞–¥–µ–ª–µ—Ü:** {rep.get('owner','')}  ¬∑  **–ß–∞—Å—Ç–æ—Ç–∞:** {rep.get('frequency','')}  ¬∑  **–°—Ç–∞—Ç—É—Å:** {'–ê–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω' if rep.get('is_automated',False) else '–†—É—á–Ω–æ–π'}")
    st.caption(rep.get("description",""))

    rf = report_fields[report_fields["report_id"]==rid].copy()
    rf["–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ –æ—Ç—á—ë—Ç–∞"] = rep["name"]
    rf = rf.rename(columns={
        "report_id":"–ö–æ–¥ –æ—Ç—á—ë—Ç–∞",
        "business_field_name":"–ë–∏–∑–Ω–µ—Å-–ø–æ–ª–µ",
        "business_algorithm":"–ë–∏–∑–Ω–µ—Å-–∞–ª–≥–æ—Ä–∏—Ç–º",
        "source_ref":"–ò—Å—Ç–æ—á–Ω–∏–∫ (schema.table.column)",
        "is_from_vitrine":"–ò–∑ –≤–∏—Ç—Ä–∏–Ω—ã?"
    })
    # –í–ê–ñ–ù–û: –≤–µ—Ä–Ω—É—Ç—å –ò—Å—Ç–æ—á–Ω–∏–∫, –∏ –ø–æ—Å—Ç–∞–≤–∏—Ç—å –µ–≥–æ –ü–û–°–õ–ï "–ò–∑ –≤–∏—Ç—Ä–∏–Ω—ã?"
    cols_order = ["–ë–∏–∑–Ω–µ—Å-–ø–æ–ª–µ","–ë–∏–∑–Ω–µ—Å-–∞–ª–≥–æ—Ä–∏—Ç–º","–ö–æ–¥ –æ—Ç—á—ë—Ç–∞","–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ –æ—Ç—á—ë—Ç–∞","–ò–∑ –≤–∏—Ç—Ä–∏–Ω—ã?","–ò—Å—Ç–æ—á–Ω–∏–∫ (schema.table.column)"]
    rf = rf[[c for c in cols_order if c in rf.columns]]

    attr_filter = st.text_input("–§–∏–ª—å—Ç—Ä –ø–æ –∞—Ç—Ä–∏–±—É—Ç–∞–º (–ø–æ–ª–µ/–∞–ª–≥–æ—Ä–∏—Ç–º/–∏—Å—Ç–æ—á–Ω–∏–∫)")
    if attr_filter:
        ql = attr_filter.lower()
        def safe_contains(s): return s.astype(str).str.lower().str.contains(ql)
        mask = pd.Series([False]*len(rf))
        for c in rf.columns:
            mask = mask | safe_contains(rf[c])
        rf_view = rf[mask]
    else:
        rf_view = rf

    st.dataframe(rf_view, use_container_width=True, height=280)
    download_button_for_df(rf_view, "‚¨áÔ∏è –°–∫–∞—á–∞—Ç—å –∞—Ç—Ä–∏–±—É—Ç—ã –æ—Ç—á—ë—Ç–∞ (Excel)", f"report_{rid}_attrs.xlsx")

# ------------------- TAB 5: –õ–∏–Ω–µ–π–Ω–æ—Å—Ç—å -----------------------------------
with tab5:
    st.subheader("Data Lineage")

    report_options = ["‚Äî –í—Å–µ –æ—Ç—á—ë—Ç—ã ‚Äî"] + reports["name"].dropna().astype(str).unique().tolist()
    selected_report_name = st.selectbox("–û—Ç—á—ë—Ç –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é ‚Äî –≤—Å–µ)", options=report_options, index=0)

    colf1, colf2 = st.columns(2)
    with colf1:
        layer_filter = st.multiselect("–°–ª–æ–π", ["vitrine","raw"], default=["vitrine","raw"])
    with colf2:
        systems = sorted(datasets["system"].dropna().astype(str).unique().tolist()) if "system" in datasets.columns else []
        system_filter = st.multiselect("–°–∏—Å—Ç–µ–º–∞", systems, default=systems)

    edges_all = build_lineage_edges(dataset_fields, report_fields)
    edges = filter_edges_by_report_name(edges_all, reports, None if selected_report_name == "‚Äî –í—Å–µ –æ—Ç—á—ë—Ç—ã ‚Äî" else selected_report_name)

    if layer_filter or system_filter:
        allowed_ds = set(
            datasets[(datasets["layer"].isin(layer_filter)) & (datasets["system"].isin(system_filter))]["name"].astype(str).tolist()
        ) if not datasets.empty else set()
        def edge_ok(s, t):
            def is_dataset(n): return isinstance(n, str) and (n.count(".")==1)
            if is_dataset(s) and allowed_ds and s not in allowed_ds:
                return False
            return True
        edges = [e for e in edges if edge_ok(e[0], e[1])]

    viz = st.radio("–¢–∏–ø –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏", ["–ì—Ä–∞—Ñ (–∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π)", "Sankey"], horizontal=True)

    if viz == "–ì—Ä–∞—Ñ (–∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π)":
        net = pyvis_graph(edges, reports, datasets, height="650px")
        html = None
        if hasattr(net, "generate_html"):
            try:
                html = net.generate_html(notebook=False)
            except Exception:
                html = None
        if html is None:
            tmp_path = "lineage_tmp.html"
            try:
                net.write_html(tmp_path)
                with open(tmp_path, "r", encoding="utf-8") as f:
                    html = f.read()
            except Exception:
                html = "<html><body><p>–ù–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –≥—Ä–∞—Ñ PyVis.</p></body></html>"

        st.caption(f"–ü–æ–∫–∞–∑–∞–Ω–æ: {selected_report_name if selected_report_name!='‚Äî –í—Å–µ –æ—Ç—á—ë—Ç—ã ‚Äî' else '–≤—Å–µ –æ—Ç—á—ë—Ç—ã'}")
        components.html(html, height=680, scrolling=True)
        st.download_button(
            "‚¨áÔ∏è –°–∫–∞—á–∞—Ç—å –≥—Ä–∞—Ñ (HTML)",
            data=html.encode("utf-8"),
            file_name=f"lineage_{'all' if selected_report_name=='‚Äî –í—Å–µ –æ—Ç—á—ë—Ç—ã ‚Äî' else selected_report_name}.html",
            mime="text/html"
        )
    else:
        fig = sankey_figure(edges)
        st.caption(f"–ü–æ–∫–∞–∑–∞–Ω–æ: {selected_report_name if selected_report_name!='‚Äî –í—Å–µ –æ—Ç—á—ë—Ç—ã ‚Äî' else '–≤—Å–µ –æ—Ç—á—ë—Ç—ã'}")
        st.plotly_chart(fig, use_container_width=True)

# ------------------- TAB 6: –ò–º–ø–æ—Ä—Ç/–≠–∫—Å–ø–æ—Ä—Ç (login admin / 321) -------------
with tab6:
    st.subheader("–ò–º–ø–æ—Ä—Ç/–≠–∫—Å–ø–æ—Ä—Ç")

    if not st.session_state.is_admin:
        colu1, colu2 = st.columns(2)
        with colu1:
            login = st.text_input("–õ–æ–≥–∏–Ω")
        with colu2:
            pwd = st.text_input("–ü–∞—Ä–æ–ª—å", type="password")
        if st.button("–í–æ–π—Ç–∏"):
            if login.strip()=="admin" and pwd=="321":
                st.session_state.is_admin = True
                st.success("–î–æ—Å—Ç—É–ø —Ä–∞–∑—Ä–µ—à—ë–Ω.")
            else:
                st.error("–ù–µ–≤–µ—Ä–Ω—ã–µ –ª–æ–≥–∏–Ω –∏–ª–∏ –ø–∞—Ä–æ–ª—å.")
        st.info("–î–æ—Å—Ç—É–ø –∫ –∑–∞–≥—Ä—É–∑–∫–µ –¥–∞–Ω–Ω—ã—Ö ‚Äî —Ç–æ–ª—å–∫–æ –¥–ª—è –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä–∞.")
    else:
        st.success("–í—ã –≤–æ—à–ª–∏ –∫–∞–∫ admin.")

        st.markdown("#### –®–∞–±–ª–æ–Ω—ã –¥–ª—è –≤—ã–≥—Ä—É–∑–∫–∏")
        template_datasets = pd.DataFrame([{
            "dataset_id":"","name":"","layer":"","owner":"","sla_minutes":"","pii_flags":"","quality_score":"","granularity":"","system":""
        }])
        template_dataset_fields = pd.DataFrame([{
            "dataset_id":"","schema":"","table":"","column":"","dtype":"",
            "completeness":"","uniqueness":"","tags":"—Å–ø–∏—Å–æ–∫_—á–µ—Ä–µ–∑_–∑–∞–ø—è—Ç—É—é",
            "business_field_name":"","business_algorithm":""
        }])
        template_reports = pd.DataFrame([{
            "report_id":"","name":"","owner":"","frequency":"","business_domain":"",
            "is_automated":"","automation_score":"","description":""
        }])
        template_report_fields = pd.DataFrame([{
            "report_id":"","business_field_name":"","business_algorithm":"",
            "source_ref":"","is_from_vitrine":""
        }])

        c1, c2, c3, c4 = st.columns(4)
        with c1: download_button_for_df(template_datasets, "‚¨áÔ∏è –®–∞–±–ª–æ–Ω: datasets.xlsx", "datasets_template.xlsx")
        with c2: download_button_for_df(template_dataset_fields, "‚¨áÔ∏è –®–∞–±–ª–æ–Ω: dataset_fields.xlsx", "dataset_fields_template.xlsx")
        with c3: download_button_for_df(template_reports, "‚¨áÔ∏è –®–∞–±–ª–æ–Ω: reports.xlsx", "reports_template.xlsx")
        with c4: download_button_for_df(template_report_fields, "‚¨áÔ∏è –®–∞–±–ª–æ–Ω: report_fields.xlsx", "report_fields_template.xlsx")

        st.markdown("---")
        st.markdown("#### –ò–º–ø–æ—Ä—Ç Excel")
        st.caption("–ó–∞–≥—Ä—É–∑–∏—Ç–µ Excel-—Ñ–∞–π–ª—ã. –¢–µ–≥–∏ –º–æ–∂–Ω–æ —É–∫–∞–∑—ã–≤–∞—Ç—å —á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é.")

        up1 = st.file_uploader("datasets.xlsx", type=["xlsx"])
        up2 = st.file_uploader("dataset_fields.xlsx", type=["xlsx"])
        up3 = st.file_uploader("reports.xlsx", type=["xlsx"])
        up4 = st.file_uploader("report_fields.xlsx", type=["xlsx"])

        if st.button("–ó–∞–º–µ–Ω–∏—Ç—å —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –Ω–∞ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–µ"):
            try:
                if up1:
                    st.session_state.datasets = pd.read_excel(up1)
                if up2:
                    df = pd.read_excel(up2)
                    if "tags" in df.columns:
                        df["tags"] = df["tags"].apply(lambda x: [t.strip() for t in str(x).split(",")] if pd.notna(x) else [])
                    st.session_state.dataset_fields = df
                if up3:
                    st.session_state.reports = pd.read_excel(up3)
                if up4:
                    st.session_state.report_fields = pd.read_excel(up4)

                # –û–±–Ω–æ–≤–∏–º –ª–æ–∫–∞–ª—å–Ω—ã–µ —Å—Å—ã–ª–∫–∏
                datasets = st.session_state.datasets
                reports = st.session_state.reports
                report_fields = st.session_state.report_fields
                dataset_fields = st.session_state.dataset_fields

                # –ü–µ—Ä–µ—Å–æ–±–µ—Ä—ë–º —Å–ª—É–∂–µ–±–Ω—ã–µ –ø–æ–ª—è/–∏–Ω–¥–µ–∫—Å
                if {"schema","table","column"}.issubset(dataset_fields.columns):
                    dataset_fields["ref"] = dataset_fields["schema"].astype(str) + "." + dataset_fields["table"].astype(str) + "." + dataset_fields["column"].astype(str)
                else:
                    dataset_fields["ref"] = ""
                st.session_state.dataset_fields = dataset_fields

                # –ü–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è –ø–æ–∏—Å–∫–∞ ‚Äî –ö–õ–Æ–ß–ï–í–û–ï, —á—Ç–æ–±—ã –ø–æ–∏—Å–∫ –Ω–µ –ª–æ–º–∞–ª—Å—è –ø–æ—Å–ª–µ –∑–∞–º–µ–Ω—ã –¥–∞–Ω–Ω—ã—Ö
                st.session_state.vectorizer, st.session_state.tfidf = build_search_index(dataset_fields, datasets)

                st.success("–î–∞–Ω–Ω—ã–µ –æ–±–Ω–æ–≤–ª–µ–Ω—ã –∏ –∏–Ω–¥–µ–∫—Å –ø–µ—Ä–µ—Å—Ç—Ä–æ–µ–Ω. –ü–µ—Ä–µ–π–¥–∏—Ç–µ –Ω–∞ –¥—Ä—É–≥–∏–µ –≤–∫–ª–∞–¥–∫–∏.")
            except Exception as e:
                st.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏: {e}")
